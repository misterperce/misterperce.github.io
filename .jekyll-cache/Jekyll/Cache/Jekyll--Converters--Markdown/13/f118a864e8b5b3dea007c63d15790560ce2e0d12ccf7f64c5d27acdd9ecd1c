I".<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ul>
      <li><a href="#span" id="markdown-toc-span">Span</a></li>
      <li><a href="#linearly-dependent" id="markdown-toc-linearly-dependent">Linearly Dependent</a></li>
      <li><a href="#basis" id="markdown-toc-basis">Basis</a></li>
      <li><a href="#linear-transformation" id="markdown-toc-linear-transformation">Linear transformation</a></li>
    </ul>
  </li>
  <li><a href="#matrix-multiplication" id="markdown-toc-matrix-multiplication">Matrix Multiplication</a></li>
  <li><a href="#determinants" id="markdown-toc-determinants">Determinants</a></li>
  <li><a href="#linear-system-of-equations" id="markdown-toc-linear-system-of-equations">Linear System of Equations</a></li>
  <li><a href="#rank" id="markdown-toc-rank">Rank</a>    <ul>
      <li><a href="#column-space" id="markdown-toc-column-space">Column Space</a></li>
      <li><a href="#null-space" id="markdown-toc-null-space">Null Space</a></li>
    </ul>
  </li>
  <li><a href="#non-square-matricies" id="markdown-toc-non-square-matricies">Non-square Matricies</a></li>
  <li><a href="#dot-product-intuition" id="markdown-toc-dot-product-intuition">Dot Product Intuition</a></li>
  <li><a href="#cross-product" id="markdown-toc-cross-product">Cross Product</a>    <ul>
      <li><a href="#cramers-rule" id="markdown-toc-cramers-rule">Cramer‚Äôs Rule</a></li>
    </ul>
  </li>
  <li><a href="#change-of-basis" id="markdown-toc-change-of-basis">Change of Basis</a></li>
  <li><a href="#eigenvectors--eigenvalues" id="markdown-toc-eigenvectors--eigenvalues">Eigenvectors &amp; Eigenvalues</a></li>
  <li><a href="#abstract-vector-spaces" id="markdown-toc-abstract-vector-spaces">Abstract Vector Spaces</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<h4 id="span">Span</h4>
<p>Set of space achievable by scaling a given set of vectors.</p>

<h4 id="linearly-dependent">Linearly Dependent</h4>
<p>Describes a vector that is redundant in a set of vectors, i.e. does not expand the span of the set of vectors. In contrast, ‚Äúlinearly independent‚Äù describes a vector that is additive to the span created by a set of vectors.</p>

<p>Linearly dependent:  \(\vec{u} = a\vec{v} + b\vec{w}\)</p>

<p>Linearly independent: \(\vec{u} \not = a\vec{v} + b\vec{w}\)</p>

<h4 id="basis">Basis</h4>
<p>The basis of a vector space is a set of linearly independent vectors that span the full space.</p>

<p>We use \(\hat{i}\) (unit vector along x axis) and \(\hat{j}\) (unit vector along y axis) as conventional basis vectors.</p>

<h4 id="linear-transformation">Linear transformation</h4>
<p>Requires that (1) the transformation preserves lines (i.e. all lines stay lines, and don‚Äôt get turned into curves) and (2) preserves the origin)</p>

\[\large{\begin{bmatrix} \textcolor{cadetblue}{a} &amp; \textcolor{DarkSlateBlue}{b} \\ \textcolor{cadetblue}{c} &amp; \textcolor{DarkSlateBlue}{d} \end{bmatrix} \begin{bmatrix} \textcolor{IndianRed}{x} \\ \textcolor{IndianRed}{y} \end{bmatrix} = \textcolor{IndianRed}{x} \begin{bmatrix} \textcolor{cadetblue}{a} \\ \textcolor{cadetblue}{c} \end{bmatrix} + \textcolor{IndianRed}{y} \begin{bmatrix} \textcolor{DarkSlateBlue}{b} \\ \textcolor{DarkSlateBlue}{d} \end{bmatrix} = \begin{bmatrix} \textcolor{cadetblue}{a}\textcolor{IndianRed}{x} +  \textcolor{DarkSlateBlue}{b}\textcolor{IndianRed}{y} \\ \textcolor{cadetblue}{c}\textcolor{IndianRed}{x} + \textcolor{DarkSlateBlue}{d}\textcolor{IndianRed}{y} \end{bmatrix}}\]

<p>Linear transformation of linearly dependent vectors will condense entire 2d coordinate plane into a line.</p>

<h2 id="matrix-multiplication">Matrix Multiplication<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup></h2>

\[\large{\overleftarrow{\overbrace{
  \begin{bmatrix}
    \textcolor{cadetblue}{a} &amp; \textcolor{cadetblue}{b} \\ \textcolor{cadetblue}{c} &amp; \textcolor{cadetblue}{d}
  \end{bmatrix}}^{\text{M2}}
\overbrace{\begin{bmatrix}
    \textcolor{DarkSlateBlue}{e} &amp; \textcolor{IndianRed}{f} \\ \textcolor{DarkSlateBlue}{g} &amp; \textcolor{IndianRed}{h}
  \end{bmatrix}}^{\text{M1}}} =
\begin{bmatrix}
  \textcolor{cadetblue}{a}\textcolor{DarkSlateBlue}{e} +  \textcolor{cadetblue}{b}\textcolor{DarkSlateBlue}{g} &amp; \textcolor{cadetblue}{a}\textcolor{IndianRed}{f} +  \textcolor{cadetblue}{b}\textcolor{IndianRed}{h} \\ \textcolor{cadetblue}{c}\textcolor{DarkSlateBlue}{e} + \textcolor{cadetblue}{d}\textcolor{DarkSlateBlue}{g} &amp; \textcolor{cadetblue}{c}\textcolor{IndianRed}{f} + \textcolor{cadetblue}{d}\textcolor{IndianRed}{h}
\end{bmatrix}}\]

<p>I added an arrow here to remind myself that we read matrix transformations, or multiplications, right to left!</p>

<h2 id="determinants">Determinants</h2>

<p>Determinants measure how area is squished or expanded through a transformation.</p>

<p>Negative determinants means that orientation of space has been inverted (i.e. the plane has flipped); absolute value of determinant will show how much the area of the space has transformed.</p>

\[det\begin{pmatrix}\begin{bmatrix} \textcolor{cadetblue}{a} &amp; \textcolor{DarkSlateBlue}{b} \\ \textcolor{cadetblue}{c} &amp; \textcolor{DarkSlateBlue}{d} \end{bmatrix}\end{pmatrix} = \textcolor{cadetblue}{a}\textcolor{DarkSlateBlue}{d} - \textcolor{DarkSlateBlue}{b}\textcolor{cadetblue}{c}\]

<p>And for 3d transformations:</p>

\[det\begin{pmatrix}\begin{bmatrix}
  \textcolor{cadetblue}{a} &amp; \textcolor{DarkSlateBlue}{b} &amp; \textcolor{IndianRed}{c} \\
  \textcolor{cadetblue}{d} &amp; \textcolor{DarkSlateBlue}{e} &amp; \textcolor{IndianRed}{f} \\
  \textcolor{cadetblue}{g} &amp; \textcolor{DarkSlateBlue}{h} &amp; \textcolor{IndianRed}{i}\end{bmatrix}\end{pmatrix}  =
  \textcolor{cadetblue}{a} \ det\begin{pmatrix}\begin{bmatrix} \textcolor{DarkSlateBlue}{e} &amp; \textcolor{IndianRed}{f} \\ \textcolor{DarkSlateBlue}{h} &amp; \textcolor{IndianRed}{i} \end{bmatrix}\end{pmatrix}
  - \textcolor{DarkSlateBlue}{b} \ det\begin{pmatrix}\begin{bmatrix} \textcolor{cadetblue}{d} &amp; \textcolor{IndianRed}{f} \\ \textcolor{cadetblue}{g} &amp; \textcolor{IndianRed}{i} \end{bmatrix}\end{pmatrix}
  + \textcolor{IndianRed}{c} \ det\begin{pmatrix}\begin{bmatrix} \textcolor{cadetblue}{d} &amp; \textcolor{DarkSlateBlue}{e} \\ \textcolor{cadetblue}{g} &amp; \textcolor{DarkSlateBlue}{h} \end{bmatrix}\end{pmatrix}\]

<h2 id="linear-system-of-equations">Linear System of Equations</h2>

<p>Identity matrix: \(\begin{bmatrix}
   1 &amp; 0 \\
   0 &amp; 1
\end{bmatrix}\) = the transformation matrix that does nothing!</p>

<p>Inverse matrix \(A^{-1}\) is such that \(A^{-1}A = \begin{bmatrix}
   1 &amp; 0 \\
   0 &amp; 1
\end{bmatrix}\)</p>

<p>Given a system of equations, you can use the identity matrix to solve for variables, as long as \(det(A) \not = 0\):</p>

\[\large \begin{aligned}2x + 2y &amp;= -4 \\ 1x + 3y &amp;= -1 \\ \underbrace{\begin{bmatrix} 2 &amp; 2 \\ 1 &amp; 3 \end{bmatrix}}_{A} \underbrace{\begin{bmatrix} x \\ y \end{bmatrix}}_{\vec{x}} &amp;= \underbrace{\begin{bmatrix} -4 \\ -1 \end{bmatrix}}_{\vec{v}} \\ A\vec{x}&amp;=\vec{v} \\ A^{-1}A\vec{x} &amp;= A^{-1}\vec{v} \\ \vec{x} &amp;= A^{-1}\vec{v}
\end{aligned}\]

<h2 id="rank">Rank</h2>

<p>Rank: number of dimensions in the output of a transformation, or number of dimensions in the column space.</p>

<p>When output of a transformation is a line, it is rank 1. When the output is a plane, it is rank 2.</p>

<p>For example, rank 2 is the highest rank a 2d coordinate system of vectors can achieve.</p>

<p>When the rank is the highest it can be, it is ‚Äúfull rank.‚Äù</p>

<h3 id="column-space">Column Space</h3>

<p>Column Space of \(A\): set of all possible outputs of \(A\vec{v}\)</p>

<p>The column space lets us understand if a solution set exists for a system of equations.</p>

<h3 id="null-space">Null Space</h3>

<p>When a transformation is full rank,  \(\begin{bmatrix} 0 \\ 0 \end{bmatrix}\) is the only vector that ends up on \(\begin{bmatrix} 0 \\ 0 \end{bmatrix}\). But, when a transformation loses rank, then multiple vectors are transformed to \(\begin{bmatrix} 0 \\ 0 \end{bmatrix}\). These vectors that have collapsed to \(\begin{bmatrix} 0 \\ 0 \end{bmatrix}\) are called the ‚Äúnull space‚Äù or ‚Äúkernel.‚Äù</p>

<p>The null space lets us understand what the set of all possible solutions can look like.</p>

<h2 id="non-square-matricies">Non-square Matricies</h2>

<ul>
  <li>Columns indicate basis vectors / input space</li>
  <li>Rows indicate ‚Äúlanding spots‚Äù for basis vectors</li>
</ul>

<h2 id="dot-product-intuition">Dot Product Intuition</h2>

\[\begin{aligned}\text{Matrix-vector product:} \begin{bmatrix}u_x &amp; u_y\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} &amp;= u_x \cdot x + u_y \cdot y \\ \text{Dot product:} \begin{bmatrix}u_x \\ u_y\end{bmatrix} \begin{bmatrix}x \\ y\end{bmatrix} &amp;= u_x \cdot x + u_y \cdot y\end{aligned}\]

<h2 id="cross-product">Cross Product</h2>

\[\vec{v} √ó \vec{w} = \vec{p}\]

<p>\(\vec{p}\) is perpendicular to the parallelogram created by \(\vec{v}\) and \(\vec{w}\) and \(\vec{p}\) has a length equal to the area of the parallelogram. Which perpendicular direction? Use the right hand rule.</p>

<p><img src="/assets/post-imgs/right_hand_rule_stolen_from_wikipedia.png" width="250" /></p>

<h3 id="cramers-rule">Cramer‚Äôs Rule</h3>

<p>See <a href="https://www.chilimath.com/lessons/advanced-algebra/cramers-rule-with-three-variables/">https://www.chilimath.com/lessons/advanced-algebra/cramers-rule-with-three-variables/</a></p>

<h2 id="change-of-basis">Change of Basis</h2>

\[A^{-1}MA \vec{v}\]

\[A^{-1} : \text{inverse change of basis matrix} \\
M: \text{transformation matrix, written in familiar basis terms} \\
A: \text{change of basis matrix} \\
\vec{v} : \text{vector in another basis}\]

<h2 id="eigenvectors--eigenvalues">Eigenvectors &amp; Eigenvalues</h2>
<p>Eigenvectors are lines(?) in which vectors are simply scaled rather than knocked off their span from a matrix transformation.</p>

<p>Eigenvalues are the scalar that the eigenvectors change by through the matrix transformation.</p>

<p>Eigenvector of a 3d rotation == axis of the rotation!!</p>

\[A\vec{v} = \lambda \vec{v}\]

<p>\(A\) is the transformation matrix; \(\vec{v}\) is the eigenvector; \(\lambda\) is the eigenvalue</p>

<p>Interpretation: matrix multiplication gives the same result as just scaling the vector with some value</p>

\[\begin{aligned} A\vec{v} &amp;= \lambda \vec{v} \\ A\vec{v} - \lambda I\vec{v} &amp;= 0 \\ (A- \lambda I)\vec{v} &amp;= 0 \\  \det(A- \lambda I) &amp; = 0\end{aligned}\]

<p>A set of basis vectors that are also eigenvectors is an eigenbasis.</p>

<h2 id="abstract-vector-spaces">Abstract Vector Spaces</h2>

<p>Functions are vector-ish and can be transformed linearly.</p>

<p>Derivatives are linear transformations.</p>

<p>Linear algebra concepts have direct analogs in the world of functions:</p>

<table>
  <thead>
    <tr>
      <th>Linear Algebra Concepts</th>
      <th>Analog to functions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Linear transformations</td>
      <td>Linear operations</td>
    </tr>
    <tr>
      <td>Dot products</td>
      <td>Inner products</td>
    </tr>
    <tr>
      <td>Eignvectors</td>
      <td>Eigenfunctions</td>
    </tr>
  </tbody>
</table>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><br /> <a href="https://www.youtube.com/watch?v=XkY2DOUCWMU&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&amp;index=4&amp;ab_channel=3Blue1Brown"><img src="/assets/post-imgs/3b1b_matrix_mult.PNG" alt="img" /></a>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET